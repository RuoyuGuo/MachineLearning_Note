{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Three step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1, Function set(Model):\n",
    "\n",
    "input layers: feature X\n",
    "\n",
    "hidden layer:\n",
    "* input: output of previous layer\n",
    "* Neuron: each neuron is a function, takes all input and generate one number, then put it to activation function to generate output \n",
    "* output: output of each neuron\n",
    "* structure: fully connect...etc\n",
    "\n",
    "output layers:\n",
    "* intput: output of last hidden layer\n",
    "* function: self designed\n",
    "* output: required output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2, Loss function:\n",
    "\n",
    "self-designed Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3, find the best function:\n",
    "\n",
    "Gradient: \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial L}{\\partial w_1} \\\\\n",
    "    \\frac{\\partial L}{\\partial w_2} \\\\\n",
    "    \\frac{\\partial L}{\\partial w_3} \\\\  \n",
    "    ... \\\\\n",
    "    \\frac{\\partial L}{\\partial b_1} \\\\\n",
    "    \\frac{\\partial L}{\\partial b_2} \\\\\n",
    "    ... \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to compute gradient (partial derivative)?\n",
    "\n",
    "* parameters: $\\theta = \\{w_1, w_2, w_3,..., b_1, b_2,...\\}$\n",
    "\n",
    "* our work: \n",
    "\n",
    "    $l^1$ is the loss of $y^1$ and $\\hat{y^1}$ as  and so on.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L(\\theta)}{\\partial w} = \\displaystyle\\sum^{N}_{n = 1}\\frac{\\partial l^n}{\\partial w} = \\displaystyle\\sum^{N}_{n = 1}\\frac{\\partial z}{\\partial w} \\frac{\\partial l}{\\partial z}\n",
    "$$\n",
    "\n",
    "* introduce new algorithm\n",
    "\n",
    "### two steps of Backpropagation\n",
    "### step1: forword pass:\n",
    "\n",
    "compute all $\\frac{\\partial z}{\\partial w}$, where $z$ is the function in neuron. When z is linear model like $z = wx $\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial w} = \\text{coefficent of parameter w, it is x in our example}\n",
    "$$ \n",
    "\n",
    "### step 2: backwork pass:\n",
    "\n",
    "inverse our neuron network\n",
    "\n",
    "input:  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial z}\\text{, where z is in the output layer}\n",
    "$$\n",
    "\n",
    "hidden layer:\n",
    "\n",
    "* input: $w_1 \\frac{\\partial l}{\\partial z_1} + w_2 \\frac{\\partial l}{\\partial z_2}$, in here, we only need to compute partial derivative of output layer, then use it as the input of next layer\n",
    "* active function: derivative of activation function in neuron network = $\\sigma '(z)$, we use sigmoid function as example in here\n",
    "* output: partial derivative\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial z} = \\sigma '(z)[w_1 \\frac{\\partial l}{\\partial z_1} + w_2 \\frac{\\partial l}{\\partial z_2}]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful material, some basic understanding of activation functon\n",
    "\n",
    "https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\n",
    "\n",
    "https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### why acitvation function\n",
    "\n",
    "Reason 1:\n",
    "\n",
    "    image our NN doesn't have activation function.\n",
    "\n",
    "    NN could be written into $Y = W_3 \\cdot (W_1 \\cdot X + W_2 \\cdot X)$ (one hidden layer, two nueron).\n",
    "\n",
    "    Wait? This is a linear function even though you can add as many hidden layer or nueron as you want. It doesn't give you new power and we want a non-linear as well as more complex function for a difficult problem. That is deep learning for. \n",
    "\n",
    "    When we add activation function. \n",
    "\n",
    "    NN is converted to $Y = g( W_3 (\\cdot f(W_1 \\cdot x) + f(W_1 \\cdot x)))$. Now it is non-linear since our activation function $f$ and $g$ are not linear.\n",
    "    \n",
    "Reason 2:\n",
    "    \n",
    "    activation function helps us to decide which neuron should be fired and which is more important to our output. When some neuron are not activated, our NN get lighter so parameters are reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### various activation function and their problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**linear activation function**\n",
    "\n",
    "a linear function is something like $f(x) = kx + b$ because its plot is a straight line.\n",
    "\n",
    "Problem? \n",
    "* changes nothing. Same as NN without activation function.\n",
    "\n",
    "* the derivative of linear function is constant($k$ in here). Which means when we tune our parameters, it doesn't follows the change of $x$.\n",
    "\n",
    "* no bound, range is $(-inf, +inf)$\n",
    "\n",
    "**sigmoid function**\n",
    "\n",
    "$f(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "advantages:\n",
    "\n",
    "* have bound, won't blow up the activations, range is $(-1, +1)$\n",
    "\n",
    "* y changes rapidly when x in $(-2, +2)$ if you observe the plot. That makes clear distinction on x.\n",
    "\n",
    "* non-linear(it is a curve)\n",
    "\n",
    "* gradient it's not linear\n",
    "\n",
    "Problems:\n",
    "\n",
    "* Vanishing Gradient Problem, look at either end of sigmoid function. The Y values is towards to -1 or +1 and has quite litter changes. Which means, the gradient at that region is going to be small. Each time the value are passed to sigmoid function, the gradient will be decreased.\n",
    "\n",
    "**Tanh function**\n",
    "\n",
    "$f(x) = \\frac{2}{1 + e^{-2x}} -1 = 2sigmoid(2x) -1$\n",
    "\n",
    "It is actually scaled sigmoid function!\n",
    "\n",
    "advantages:\n",
    "    \n",
    "* similar to sigmoid function\n",
    "\n",
    "* gradient changes more significant than sigmoid\n",
    "\n",
    "Problems:\n",
    "\n",
    "* vanishing gradient problem\n",
    "\n",
    "**ReLu**\n",
    "\n",
    "$ \n",
    "f(x) =\n",
    "  \\begin{cases}\n",
    "     x  &  \\text{if } x >= 0 \\\\\n",
    "     0  &  \\text{if } x < 0\n",
    "  \\end{cases}\n",
    "$\n",
    "\n",
    "advantages:\n",
    "\n",
    "* It is not linear when x locate into different region (greater than 0 or less than 0)\n",
    "\n",
    "* solve the vanishing Gradient problem, since it is linear when we look at one region\n",
    "\n",
    "* make your NN sparse. When some Xs are less than 0, the output is 0. That means this neuron changes nothing to your output(fewer neurons are firing). This is actually we want because this make neurons to focus on small region and they are identical. Parameters are reduced at the same time.\n",
    "\n",
    "* fast to compuate(no exponential, less neurons are firing)\n",
    "\n",
    "problems:\n",
    "\n",
    "* the gradient of x when x less than 0 is 0. Sometime it cause some neuron doesn't work at all.\n",
    "\n",
    "**Leaky ReLu**\n",
    "\n",
    "$ \n",
    "f(x) =\n",
    "  \\begin{cases}\n",
    "     x  &  \\text{if } x >= 0 \\\\\n",
    "     0.01x  &  \\text{if } x < 0\n",
    "  \\end{cases}\n",
    "$\n",
    "\n",
    "**Parametric ReLU**\n",
    "\n",
    "$ \n",
    "f(x) =\n",
    "  \\begin{cases}\n",
    "     x  &  \\text{if } x >= 0 \\\\\n",
    "     ax  &  \\text{if } x < 0\n",
    "  \\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to choose them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sigmoid and Tanh function woks well for classifier. (see the graph of it. Y changes significantly)\n",
    "\n",
    "* ReLU works almost all scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> how to train DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have a bad result on Training data\n",
    "\n",
    "### Solution:\n",
    "* New activation function:\n",
    "    * Vanishing Gradient Problem: \n",
    "    \n",
    "    * ReLU (Rectified Linear Unit): make sure structure is linear\n",
    "    $$\n",
    "    \\sigma(z) =\n",
    "        \\begin{cases}\n",
    "          z & \\quad \\text{if } z>=0 \\\\\n",
    "          0 & \\quad \\text{if } z<0 \n",
    "        \\end{cases}             \n",
    "    $$\n",
    "    \n",
    "    * Leaky ReLU:\n",
    "    $$\n",
    "    \\sigma(z) =\n",
    "        \\begin{cases}\n",
    "          z & \\quad \\text{if } z>=0 \\\\\n",
    "          0.01z & \\quad \\text{if } z<0 \n",
    "        \\end{cases}             \n",
    "    $$\n",
    "    \n",
    "    * Parametric ReLU:\n",
    "    $$\n",
    "    \\sigma(z, \\alpha) =\n",
    "        \\begin{cases}\n",
    "          z & \\quad \\text{if } z>=0 \\\\\n",
    "          \\alpha z & \\quad \\text{if } z<0 \n",
    "        \\end{cases}             \n",
    "    $$\n",
    "    \n",
    "    * Maxout (ReLU is a special cases of Maxout):\n",
    "    \n",
    "    group output of each layer, output of activation function is the largest value in a group\n",
    "    \n",
    "* Adaptive Learning Rate:\n",
    "    * RMSProp:\n",
    "    $$\n",
    "    w^{t+1} \\leftarrow w^t - \\frac{\\eta}{\\sigma^t} g^t , \\\\\n",
    "    \\sigma^0 = g^0 ,\\\\\n",
    "    \\sigma^t = \\sqrt{\\alpha(\\sigma^{t-1})^2 + (1 - \\alpha)(g^t)^2}  \n",
    "    $$\n",
    "    \n",
    "    * Momentum:\n",
    "    $$\n",
    "    w^{t+1} = w^{t} + v^{t+1}, \\\\\n",
    "    v^0 = 0. \\\\\n",
    "    v^{t+1} = \\lambda v^t - \\eta * \\text{graident}^t\n",
    "    $$\n",
    "    \n",
    "    * Adam:\n",
    "    \n",
    "    RMSProp + Momentum\n",
    "    \n",
    "    increase training speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have a bad result on Testing data\n",
    "### Solution:\n",
    "* Early Stopping:\n",
    "Stop when Loss doesn't decrease on validation set\n",
    "\n",
    "* Regularization:\n",
    "    $$\n",
    "        L^{\\prime}(\\theta) = L(\\theta) + \\lambda\\frac{1}{2}\\|\\theta\\|,\n",
    "    $$\n",
    "    * L2: \n",
    "    $$\n",
    "        \\|\\theta\\| = (w_1)^2 + (w_2)^2 + ...\n",
    "    $$\n",
    "    * L1:\n",
    "    $$\n",
    "        \\|\\theta\\| = |w_1| + |w_2| + ..., \\\\\n",
    "        \\frac{\\partial{L}^{\\prime}}{\\partial{w}} = \\frac{\\partial{L}}{\\partial{w}} + \\lambda sgn(w)\n",
    "    $$\n",
    "    \n",
    "* Dropout:\n",
    "\n",
    "    Each neuron has p% to dropout before update parameters,\n",
    "\n",
    "    final weight should times 1 - p%\n",
    "\n",
    "    Dropout can train a lot of models and use average as final model. \n",
    "\n",
    "    Hence it has a good result on testing data\n",
    "    \n",
    "    Drop can add to any layer including cnn layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Hyper-parameters in DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/a-guide-to-an-efficient-way-to-build-neural-network-architectures-part-i-hyper-parameter-8129009f131b\n",
    "\n",
    "library:\n",
    "\n",
    "Hyperas, Hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what is Hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is configurable value that should be set before training model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Layers**\n",
    "\n",
    "* large number: over-fitting, vanishing or exploding graident problems\n",
    "\n",
    "* low number: high bias and too simple model\n",
    "\n",
    "* depending on the data size\n",
    "\n",
    "**Number of hidden units per layer**\n",
    "\n",
    "* depending on data size\n",
    "\n",
    "**Activation Function**\n",
    "\n",
    "* sigmoid and Tanh may do well for shallow networks\n",
    "\n",
    "**Optimizer**\n",
    "\n",
    "* SGD works well for shallow networks, cannot escape saddle points and local minima\n",
    "\n",
    "* Adam is the best generally\n",
    "\n",
    "* Adagrad for sparse data\n",
    "\n",
    "**Learning Rate**\n",
    "\n",
    "* try power of 10, 0.001, 0.01, 0.1, 1...\n",
    "\n",
    "**Initialization**\n",
    "\n",
    "* doesn't play a necessary role while could use He-normal/uniform initializaiton\n",
    "\n",
    "* avoid zero or any constant value\n",
    "\n",
    "**Batch Size**\n",
    "\n",
    "* try power of 2\n",
    "\n",
    "**Numer of Epochs**\n",
    "\n",
    "* ...\n",
    "\n",
    "**Dropout**\n",
    "\n",
    "* ...\n",
    "\n",
    "**L1/L2 Regularization**\n",
    "\n",
    "* L1 is stronger than L2, try coefficience with power of 10, 0.0001, 0.001...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Convolutional Neural Network,  CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can CNN do?\n",
    "\n",
    "1. recognize small region of the who image\n",
    "\n",
    "2. recognize same patterns appear in different regions (use same parameters)\n",
    "\n",
    "3. subsampling(sample a part of image) and won't change the object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution, for 1 and 2\n",
    "* filter: Each filter detects a small pattern. \n",
    "    * filter is a matrix, the product of filter and a part of image represent the content in this part. Same product means that these two part both have same content and greater number means more similar\n",
    "* stride: move distance of filter\n",
    "* output: output of A filter is a small part of image. Then the next function could use this output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Pooling, for 3\n",
    "* group the resullt of dot product of filter and part of image\n",
    "* output the max of each group\n",
    "* the max value is recognised part of the image, which could be useful for neuron network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More applications:\n",
    "* Deep Dream\n",
    "* alpha go\n",
    "* audio recognisation\n",
    "* Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what does CNN learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since We use filter to detect pattern. What the machine can learn is that what kind of picture or input can activate this filter or more likely to the pattern we want to find, let say a mouse or a eye."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Hyper-parameters in CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**kernel/Filter Size**\n",
    "\n",
    "* small filters collect local information. Use it if you think what differientiates objects are some small and local features (5 * 5, 3 * 3)\n",
    "\n",
    "* large filters look on global and high-level information. Use it if you think that a great amout of pixels are necessary for the network to recognize the object (11 * 11, 9 * 9)\n",
    "\n",
    "* generally use odd size \n",
    "\n",
    "* often use (3 * 3, 5 * 5, 7 * 7) for small-sized images\n",
    "\n",
    "\n",
    "**padding**\n",
    "\n",
    "* add zeros on columns and rows. It will keep the size of input after filtering\n",
    "\n",
    "* set 'padding = same' to enable padding when you think the border is important\n",
    "\n",
    "* default is 'padding = valid' where the output size if $\\lceil ((n-f+1)/s) \\rceil$, where n it the input dimensions, f is filter size and s is stride length\n",
    "    \n",
    "* padding for maxpooling, reference:https://www.pico.net/kb/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-tensorflow\n",
    "    * padding = 'valid',\n",
    "        * out_height\t= ceil(float(in_height - filter_height + 1) / float(strides[1]))\n",
    "        * out_width\t= ceil(float(in_width - filter_width + 1) / float(strides[2]))\n",
    "    * padding = 'same',\n",
    "        * out_height\t= ceil(float(in_height) / float(strides[1]))\n",
    "        * out_width\t= ceil(float(in_width) / float(strides[2]))\n",
    "        * pad_along_height\t= max((out_height - 1) * strides[1] + filter_height - in_height, 0)\n",
    "        * pad_along_width\t= max((out_width - 1) * strides[2] + filter_width - in_width, 0)\n",
    "        * pad_top\t= pad_along_height // 2\n",
    "        * pad_bottom\t= pad_along_height - pad_top\n",
    "        * pad_left\t= pad_along_width // 2\n",
    "        * pad_right\t= pad_along_width - pad_left\n",
    "\n",
    "**stride**\n",
    "\n",
    "* 1 or 2\n",
    "\n",
    "**Number of Channels/filters**\n",
    "\n",
    "* It means the number of color channels for a image(3 for RGB) but in CNN it is the number of filters\n",
    "\n",
    "* greater number, more features learnt, more chances to over-fit or vice-versa\n",
    "\n",
    "* should be low in the beginning such that it detects low-level features then increase\n",
    "\n",
    "* start by using small value then gradually increase to reduce the generated feature space width\n",
    "\n",
    "* or stay the same\n",
    "\n",
    "* 32-64-128... or 32-32-64-64...\n",
    "\n",
    "**Pooling-layer parameters**\n",
    "\n",
    "* 2 * 2 or 3 * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* keep adding until over-fit, then try *dropout, regularization, batch norm, data augmentation...*\n",
    "\n",
    "* try classic networks like LeNet, AlexNet, VGG-16, VGG-19 etc...\n",
    "\n",
    "* try Conv-Pool-Conv-Pool or try Conv-Conv-Pool-Conv-Conv-Pool "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Why Deep Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Deep Learning uses less paramters, less data\n",
    "* Modularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1805.11604.pdf\n",
    "\n",
    "**Recall:**\n",
    "\n",
    "Normalization can push data in similar region to eliminate influence of scale and boost training speed.\n",
    "\n",
    "Batch Normalization is a normalization to centre data by subtracting mean, then resualts are divided by standard diviation. After doing BN, noramlised data has mean with 0 and standard divation with 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In NN\n",
    "\n",
    "**where to use them?**\n",
    "\n",
    "Typically, BN use whole batch to compute mean and standard diviation. Since it normalize the input data, we need to apply them between each layer(after or before activation function). \n",
    "\n",
    "If you use it before acitivation function, it may mitigate vanishing gradients problem because data is located in central region(imagin the plot of sigmoid function).\n",
    "\n",
    "**How to do backpropagation**\n",
    "\n",
    "Let's look at a simple NN, only one hidden layer $W, B$ and inputs are $X$.\n",
    "\n",
    "The training process is:\n",
    "1. $Z_1 = WX + B$\n",
    "2. $Z_2 = \\frac{Z_1 - \\mu_{x}}{\\sigma_{x}}$, where $\\mu = \\frac{1}{m} \\sum_i^m x_i$, and $\\sigma = SD$. This tell us $\\mu$ and $sigma$ depend on x.\n",
    "3. $Y = f(Z_2)$, where $f()$ is a activation function\n",
    "\n",
    "Hence, when we do backpropagation, $\\mu$ and $\\sigma$ should be considered.\n",
    "\n",
    "**Tips**\n",
    "\n",
    "* use less dropout probability\n",
    "\n",
    "* for those activation function has better performance on larger mean or SD, we can expand NN.\n",
    "    * using the example given above.\n",
    "    * keep $Z_1$, $Z_2$\n",
    "    * add one more layar, $Z_3 = \\gamma \\cdot Z_2 + \\beta$\n",
    "    * this new layer can change mean and SD.\n",
    "    * Don't need to worry that $\\gamma$ and $\\beta$ are the same as $\\mu$ and $\\sigma$ because $\\gamma$ and $\\beta$ are independant on $X$. In other word, They are generated by learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/34716454/where-do-i-call-the-batchnormalization-function-in-keras\n",
    "\n",
    "https://www.reddit.com/r/MachineLearning/comments/67gonq/d_batch_normalization_before_or_after_relu/\n",
    "\n",
    "https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md\n",
    "\n",
    "https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* reduce training times, and make very deep net trainable\n",
    "    * use large learning rates, since BN makes the network more stable during training. This may require the use of much larger than normal learning rates. \n",
    "    * Less exploding/vanishing gradients, sigmoid, tanh, etc.\n",
    "    \n",
    "* It can be used with most network types, multilayer perceptrons, CNN, RNN...\n",
    "\n",
    "* can make training deep works less sensitive to the weights initialization.\n",
    "    * initial $W_1 = w$, $W_2 = k \\cdot w$\n",
    "    * output: $Z_1 = W_1 \\cdot X = w \\cdot X$ and $Z_2 = W_2 \\cdot X = k \\cdot w \\cdot X$, outpus are significantly different.\n",
    "    * use BN, $Z_1 = \\frac{Z_1 - \\mu_x}{\\sigma_x} = \\frac{w \\cdot X - \\mu_x}{\\sigma_x}$\n",
    "    * $Z_2 = \\frac{Z_2 - k \\cdot \\mu_x}{k \\cdot \\sigma_x} = \\frac{k \\cdot w \\cdot X - k \\cdot \\mu_x}{k \\cdot \\sigma_x}$  \n",
    "    * same results\n",
    "* reduce demand for regularization.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
