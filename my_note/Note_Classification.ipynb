{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideal process(Binary classification):\n",
    "### Step1, Function(Model):\n",
    "\n",
    "input: x\n",
    "\n",
    "intermediate output: g(x) > 0, output: class1\n",
    "\n",
    "intermediate output: g(x) < 0, output: class2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2, Loss function:\n",
    "\n",
    "$ L(f) = \\displaystyle\\sum_n\\delta(f(x^n) \\neq \\hat{y}^n) $ \n",
    "\n",
    "is the number of times f get incorrect results on training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3, find the best function:\n",
    "\n",
    "how?\n",
    "\n",
    "**estimating the probabilities of a sample from a class**\n",
    "\n",
    "means we want to know, the probability of x from class 1 (could be any classes) given in x is sampled\n",
    "\n",
    "$$\n",
    "P(C_1 | x) = \\frac{P(C_1, x)}{P(x)} = \\frac{P(x|C_1)P(C_1)}{P(x)}\n",
    "$$\n",
    "\n",
    "but what is the probability of x is sampled?\n",
    "\n",
    "Assume there are N classes, the probability of sample x from class 1 is the probability of choosing class 1 times the probability of sample x from class 1\n",
    "\n",
    "that is\n",
    "$$\n",
    "    P(C_1)P(x|C_1)\n",
    "$$\n",
    "\n",
    "the probability of sample x from class 2 is similar and so on for class 3,4,5...\n",
    "\n",
    "the sum of them is the probability of x is sampled\n",
    "\n",
    "$$\n",
    "P(x) = \\displaystyle\\sum_nP(x|C_n)P(C_n)\n",
    "$$\n",
    "\n",
    "\n",
    "therefore, we get\n",
    "\n",
    "$$\n",
    "P(C_1 | x) = \\frac{P(x|C_1)P(C_1)}{\\displaystyle\\sum_nP(x|C_n)P(C_n)}\n",
    "$$\n",
    "\n",
    "It is easy to know $P(C_n)$, is the percent of $C_n$,\n",
    "\n",
    "\n",
    "$$\n",
    "P(C_n) = \\frac{\\text{the number of a data from class n in training data}}{\\text{the number of training data}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> now the question is the estimating of $P(x|C_n)$, \n",
    "\n",
    "### <center> the probability of sample A data x from class n,\n",
    "\n",
    "#### <center> remember that x is feacture vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Method 1. Generative\n",
    "\n",
    "we assume data in **class n** is sampled from A specific probability model,\n",
    "\n",
    "***use any distirbution you like***\n",
    "\n",
    "**why?**\n",
    "\n",
    "the feacture of data from the same class might be regular rather than random\n",
    "\n",
    "like the height of person is often in a Gaussian distribution(Normal distribution)\n",
    "\n",
    "### example, Gaussian distribution:\n",
    "\n",
    "Gaussian distribution is defined by two parameters, $\\mu$ and $\\Sigma$,\n",
    "\n",
    "### Note:\n",
    "\n",
    "we use $\\Sigma$ rather than $\\sigma$ in here because a data is represented by vector rather than a number. \n",
    "\n",
    "So follow the variance formula, it should has some change in here\n",
    "\n",
    "$$\n",
    "Var = \\frac{1}{N} \\sum (x^n - m) ^ 2 = \\frac{1}{N} \\sum (f(x) - \\hat{y})(f(x) - \\hat{y})^T\n",
    "$$ \n",
    "\n",
    "$\\Sigma$ is called covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to find best $\\mu$ and $\\Sigma$?**\n",
    "\n",
    "**We use the Maximum Likelihood**\n",
    "\n",
    "We assume x^1, x^2, x^3,...(our data) generate from the Guassian with the maximum likelihood\n",
    "\n",
    "Set $\\mu$ and $\\Sigma$ to let the probability of sample all data in training data has the largest probability, that is \n",
    "    \n",
    "$$\n",
    "    Likelihood(\\mu, \\Sigma) = f_{\\mu,\\Sigma}(x^1)f_{\\mu,\\Sigma}(x^2)f_{\\mu,\\Sigma}(x^3)...f_{\\mu,\\Sigma}(x^n)\n",
    "$$\n",
    "\n",
    "in here,\n",
    "\n",
    "$$\n",
    "    f_{\\mu, \\Sigma}(x^n) = \\text{the probability density of x^n}\n",
    "$$\n",
    "\n",
    "we can use derivatives to find appropriate $\\mu$ and $\\Sigma$,\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{N} \\displaystyle\\sum_n x^n\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{N} \\displaystyle\\sum_n (x^n - \\mu)(x^n - \\mu)^T\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, we could do classification**\n",
    "\n",
    "for a binary classifcation,\n",
    "\n",
    "if $P(C_1|x) > 0.5  \\Rightarrow x$ belongs to class1, $P(C_2 | x) < 0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**However, performance on prediction is not good**\n",
    "\n",
    "**why?**\n",
    "\n",
    "Because for a specific class we use specific $\\mu$ and $\\Sigma\\$, this may induce overfitting (too many parameters).\n",
    "\n",
    "Hence, we use same $\\Sigma\\$ for all class.\n",
    "\n",
    "$$\n",
    "\\Sigma = \\text{Weighted Average}(\\Sigma_n) = P(C_1)\\Sigma_1 + P(C_2)\\Sigma_2 + P(C_3)\\Sigma_3 + ... \n",
    "$$\n",
    "\n",
    "**what is covariance?**[https://towardsdatascience.com/let-us-understand-the-correlation-matrix-and-covariance-matrix-d42e6b643c22]\n",
    "\n",
    "when calculate covariance, use the sum of product of difference rather than the product of difference of who matrix\n",
    "\n",
    "since the former has a higher accuracy than later\n",
    "\n",
    "**now, it has relatively better performance on prediction**\n",
    "\n",
    "### Go back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Classification of Probabilistic Generative Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1:\n",
    "#### Function Set(Model):\n",
    "\n",
    "Input: x\n",
    "\n",
    "intermediate output: $P(C_1 | x)$, \n",
    "\n",
    "for binary classification,\n",
    "\n",
    "if $P(C_1 | x)$ > 0.5, \n",
    "\n",
    "output: class 1, else class 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2:\n",
    "#### Goodness of a function:\n",
    "\n",
    "The mean $\\mu$ and covariance $\\Sigma$ that maximize the likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3:\n",
    "#### Find the best function:\n",
    "\n",
    "find the $\\mu$ and $\\Sigma$ then compute the probability.\n",
    "\n",
    "**Alternatively**\n",
    "\n",
    "**example: Gaussian distribution**\n",
    "\n",
    "$$\n",
    "P(C_1 | x) = \\frac{P(x | C_1)P(C_1)}{P(x|C_1)P(C_1) + P(x|C_2)P(C_2)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{1 + \\frac{P(x|C_2)P(C_2)}{P(x | C_1)P(C_1)}} = \\frac{1}{1 + e^{-z}} = \\sigma(z) \n",
    "$$\n",
    "\n",
    "we get a sigmoid function\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "z = \\ln \\frac{P(x | C_1)P(C_1)}{P(x|C_2)P(C_2)}\n",
    "$$,\n",
    "\n",
    "substitude P(...) with real value or distribution\n",
    "\n",
    "... \n",
    "\n",
    "we get \n",
    "\n",
    "$$\n",
    "z = w^Tx - b \n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "w^T = (\\mu^1 - \\mu^2)^T\\Sigma^{-1}\n",
    "$$ \n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "b = -\\frac{1}{2}(\\mu^1)^T\\Sigma^{-1}\\mu^1 + \\frac{1}{2}(\\mu^2)^T\\Sigma^{-1}\\mu^2 + \\ln\\frac{N_1}{N_2}\n",
    "$$\n",
    "\n",
    "where $N_1$ stands for the number of data belongs to class 1 in traning data, similar to $N_2$\n",
    "\n",
    "### It is still a linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Classification of Discriminative model,  Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1:\n",
    "#### same as generative model\n",
    "\n",
    "find $P_{w,b}(C_1 | x)$,\n",
    "\n",
    "\n",
    "if P >= 0.5, output $C_1$, else output $C_2$\n",
    "\n",
    "function set:\n",
    "\n",
    "$$\n",
    "f_{w,b}(x) = P_{w, b}(C_1 | x) = \\sigma(z)\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2:\n",
    "#### goodness of a function\n",
    "\n",
    "recall: \n",
    "\n",
    "$$\n",
    "Likelihood(w, b) = f_{w,b}(x^1)f_{w,b}(x^2)(1 - f_{w,b}(x^3))...f_{w,b}(x^n)\n",
    "$$\n",
    "\n",
    "$f_{w,b}(x^1)$ means data $x^1$ belongs to class 1\n",
    "\n",
    "$(1 - f_{w,b}(x^3))$ means data $x^3$ belongs to class 2 \n",
    "\n",
    "If we transform $Likelihood(w,b)$ to $-\\ln Likelihood(w,b)$\n",
    "\n",
    "we could see it as **A loss function** and our goal is to minimize the value of the loss function.\n",
    "\n",
    "This is for generalizing Step 2 in Machine learning.\n",
    "\n",
    "Moreover, we could same formula to represent data from different class.\n",
    "\n",
    "$$\n",
    "L(w, b) = \\displaystyle \\sum_n -[\\hat{y}^n \\ln f_{w, b}(x^n) + (1 - \\hat{y}^n) \\ln (1 - f_{w, b}(x^n))]\n",
    "$$\n",
    "\n",
    "**which is the sum of cross entropy**\n",
    "\n",
    "$\\hat{y}^n = 1$ when data belongs to class 1,\n",
    "= 0 belongs to class 2\n",
    "\n",
    "**what is cross entropy? [https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3:\n",
    "#### how to find the best function\n",
    "\n",
    "**Gradient**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = \\sum_{n}-(\\hat{y}^n - f_{w, b}(x^n))x_i^{n}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\sum_{n}-(\\hat{y}^n - f_{w, b}(x^n))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Multi-class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* define independent $w, b$ for different class\n",
    "\n",
    "    * $C_1 : w_1, b_1$\n",
    "    \n",
    "    * $C_2 : w_2, b_2$\n",
    "\n",
    "    * $C_3 : w_3, b_3$\n",
    "\n",
    "* for a input x, calculate\n",
    "\n",
    "    * $z_1 = w_1x + b_1$\n",
    "    \n",
    "    * $z_2 = w_2x + b_2$\n",
    "    \n",
    "    * $z_3 = w_3x + b_3$\n",
    "    \n",
    "* probability of a data belongs to a class:\n",
    "\n",
    "$$\n",
    "y_i = \\frac{e^{z_i}}{\\displaystyle \\sum_n{e^{z_n}}}\n",
    "$$\n",
    "\n",
    "* Loss: cross entropy\n",
    "\n",
    "$$\n",
    "-\\displaystyle \\sum_n \\hat{y}\\ln y_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y} = [1, 0, 0]^T, \\text{if x belongs to class 1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Data preprocessing\n",
    "    \n",
    "### normalization\n",
    "\n",
    "methods: https://en.wikipedia.org/wiki/Feature_scaling\n",
    "\n",
    "introduction: https://jamesmccaffrey.wordpress.com/2019/01/04/how-to-normalize-training-and-test-data-for-machine-learning/\n",
    "\n",
    "tips:\n",
    "\n",
    "* use parameters from training data, since test data is the future data, we assume these future data has the similar distribution as training data\n",
    "\n",
    "* normalize data to interval (0, 1)\n",
    "\n",
    "### one-hot Encoding\n",
    "\n",
    "introduction: https://medium.com/@michaeldelsole/what-is-one-hot-encoding-and-how-to-do-it-f0ae272f1179\n",
    "\n",
    "some data, especially in classification, is not represented a suitable way for computer to understand. Let's assume we are playing with categorical data, like dog and cat with some input features. Since computer cannot understand dog and cat but can work with number, we could label them to use 1 to represent dog, 2 to represent cat.\n",
    "\n",
    "This is the intuitive way and easy to understand. However, what happens when there are 3 animals, the 3rd one is rabbit. In this scenario, the distance from cat to other two pets is same but dog is far away from rabbit, which means dog is more like cat but less like rabbit. \n",
    "\n",
    "To solve this problem, use vecotr(binary) rather than a number to label them. like [0, 0, 1] for rabbit, [0, 1, 0] for cat.\n",
    "\n",
    "Finally, they has the same distance to each other."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
