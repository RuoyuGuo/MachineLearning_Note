{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Semi-supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supervised Learning**:\n",
    "\n",
    "training date: labelled\n",
    "\n",
    "**Semi-supervised Learning**:\n",
    "\n",
    "trainning data: labelled + unlabelled\n",
    "\n",
    "the number of unlabelled data is quite larger than labelled\n",
    "\n",
    "* transductive learning: unlabelled data is test data\n",
    "    \n",
    "* inductive learning: unlabelled data is not test data\n",
    "\n",
    "why semi-supervised learning?\n",
    "\n",
    "collecting data is easy but labelling data is expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> how to do Semi-supervised Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps:**\n",
    "\n",
    "$x^L$ = labelled data, $x^u$ = unlabelled data\n",
    "\n",
    "1. initialise, *(parameter will converges eventually, but initialisation affect final result)*\n",
    "\n",
    "\n",
    "$$\n",
    "P(C_1), P(C_2),..., \\mu^1, \\mu^2, ..., \\Sigma\n",
    "$$ \n",
    "\n",
    "2. compute the posterior probability of unlabelled data using your initialisation\n",
    "\n",
    "$$\n",
    "P(C_1 | x^u) \n",
    "$$\n",
    "\n",
    "3. update parameters\n",
    "$$\n",
    "    P(C_1) = \\frac{N_1 + \\sum{P(C_1 | x^u)}}{N}, \n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\mu^1 = \\frac{\\sum{x^L}}{N_1} + \\frac{\\sum{P(C_1 | x^u)}x^u}{\\sum{P(C_1 | x^u)}}\n",
    "$$\n",
    "\n",
    "4. then back to step 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**proof:**\n",
    "\n",
    "when we use generative model to training labelled data, we want to maximise likelihood:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\sum P(x, y)\n",
    "$$\n",
    "\n",
    "$$\n",
    ", P(x, y) = P(x|y)P(y) \n",
    "$$\n",
    "\n",
    "when it comes to labelled data + unlabelled data\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\sum P(x^L, y) + \\sum P(x^U)\n",
    "$$\n",
    "\n",
    "$$\n",
    ", P(x^U) = P(x^U|C_1)P(C_1) + P(x^U | C_2)P(C_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-density Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**what is it?**\n",
    "\n",
    "suppose unlabelled data must come from one class\n",
    "\n",
    "which means, the probablity of x from one class must be 1\n",
    "\n",
    "like, if $p(c_1|x) = 0.7$, $p(c_2|x) = 0.3$, then it should be converted to $p(c_1|x) = 1.0$ for next training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**how to do it?**\n",
    "\n",
    "**self-training**\n",
    "\n",
    "* training model to get F\n",
    "\n",
    "* use F to predict a part of unlabelled data\n",
    "\n",
    "* then put them to labelled data set\n",
    "\n",
    "* go back\n",
    "\n",
    "**entropy-based regularization**\n",
    "\n",
    "the $y^u$ we get from our model are more likely from one class.\n",
    "\n",
    "which is to minimise, where $y^u$ is ratio between the number of member in one class and the number of all member\n",
    "\n",
    "$$\n",
    "E(y^u) = - \\sum y^u \\ln(y^u)\n",
    "$$\n",
    "\n",
    "it describes the distribution of your unlabelled data, unlabelled data with larger more spread out , otherwise more concentrated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothness Assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**what is it?**\n",
    "\n",
    "* x is not uniform\n",
    "\n",
    "* if $x^1$ and $x^2$ is connected by a high density path\n",
    "\n",
    "* $y^1$ and $y^2$ are the same\n",
    "\n",
    "or more generally, if there is some data, $x^1, x^2, x^3, x^4, x^5$ and a function f such that data $x^1$ is more like $x^$ based on $f(x^1, x^2)$, same as $x^2$ and $x^3$..., finally to $x^5$, then we think $x^1$ and $x^5$ from same class \n",
    "\n",
    "**how to know whether they are connected?**\n",
    "\n",
    "* cluster and label\n",
    "\n",
    "    using the features, distribution... of all data to group unlabelled data \n",
    "\n",
    "* graph-based approach\n",
    "\n",
    "    * see data as vertex\n",
    "    * whether two vertices are connected? \n",
    "       * K nearest Neighbor\n",
    "       * e-neighborhood, draw a circle with radius of e, neighbors are the vertex within the circle\n",
    "    * define a function $s(x^i, x^j)$ to determine the similarity between two vertices\n",
    "       * common s, GRBF(Gaussian Raidal Basis Function): \n",
    "       $$\n",
    "       s(x^i, x^j) = exp(-\\gamma||x^i - x^j||^2), \\text{euclidean distance}\n",
    "       $$ \n",
    "    * s() define the weight on edge\n",
    "    * smoothness or regularization is the function below, minimise it for $i,j \\in \\text{all data}$, w is the weight defined by s\n",
    "        $$\n",
    "        S = \\frac{1}{2}\\displaystyle\\sum_{i,j}w_{i,j}(y^i-y^j)^2 = y^TLy\n",
    "        $$\n",
    "        \n",
    "        where, \n",
    "        $$\n",
    "            y = (y^1, y^2, y^3...)^T, (features, real + unlabel) dim\n",
    "        $$\n",
    "        \n",
    "        $$\n",
    "            T = D - W\n",
    "        $$\n",
    "        \n",
    "        W is the matrix to represent the weighted graph,\n",
    "        \n",
    "        D the also a matrix,  defined as \n",
    "\n",
    "        $$\n",
    "            x^D_{i, j} = \\displaystyle\\sum_{m} x^w_{i, m}, \\text{where } i = j\n",
    "        $$\n",
    "               \n",
    "        if $i \\ne j$ , $X^D_{i, j}$ = 0\n",
    "        \n",
    "        $x^D$ is the element in matrix D,  $x^w$ is the element in matrix W, \n",
    "    * $Loss = \\sum c(y, \\hat{y}) + \\lambda S$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
