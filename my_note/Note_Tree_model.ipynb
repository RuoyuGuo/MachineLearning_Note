{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree model\n",
    "\n",
    "reference:\n",
    "\n",
    "https://www.ke.tu-darmstadt.de/lehre/archiv/ws0809/mldm/dt.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "* given a data set S contain all data\n",
    "\n",
    "* If all data in S belong to the same class c\n",
    "\n",
    "    * return a tree with single node having label c\n",
    "\n",
    "* else, select an attribute $A_i$ according to some heuristic function\n",
    "\n",
    "* split data into D groups, $D =$ domain size of $A_i$ \n",
    "\n",
    "* create edges from root node($A_i$) to each groups.\n",
    "\n",
    "* for each groups, recursivly use this algorithm to generate decision tree\n",
    "\n",
    "    * for each group, data set $S =$ data in that group "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristic function\n",
    "\n",
    "**How to select attribute to split data?**\n",
    "\n",
    "We always want an attribute can split data such that class in each group as pure as possible. That is, data in one group have as less different class as possible. All in one class is the best.\n",
    "\n",
    "Therefore, we first choose some attribute can discriminate data as much as possible, and last choose some attribute cannot discriminate data(no information can gain from that attribute).\n",
    "\n",
    "**How to know if an attribute is discriminated or informative?**\n",
    "\n",
    "* Entropy:\n",
    "\n",
    "    * entropy tell us how data distributed in data set.\n",
    "    \n",
    "    * large entropy, data is randomly distributed, small entropy, data distribution follow some rule\n",
    "    \n",
    "    * $E(S) = \\sum_i-p_i\\log_2{p_i}$, where $S$ is data set, $i$ represent class(label) in set $S$, $p_i$ is the proportion of class $i$ in $S$. Normally, $S$ is all data has the same value in one attribute, therefore, $S = \\sum_{A_i = a} x$, THat is all data has value $a$ on attribute $A_i$\n",
    "    \n",
    "* Average entropy:\n",
    "\n",
    "    * weighted sum entropy of an attribute.\n",
    "    \n",
    "    * select the attribute has the smallest average entropy\n",
    "    \n",
    "    * $AE(S, A) = \\sum_i \\frac{|S_i|}{|S|}E(S_i)$, where $S$ is data set, $S_i$ is all data in $S$ has value $i$ on attribute $A$\n",
    "    \n",
    "* Information Gain:\n",
    "\n",
    "    * tell us how much information we can gain from that attribute\n",
    "    \n",
    "    * used as heuristic function when we select attribute\n",
    "    \n",
    "    * we always select the attribute has the largest information gain\n",
    "    \n",
    "    * we only need to calculate $AE(S, A)$ actually, because $E(S)$ is equal for all attribute.\n",
    "    \n",
    "    * $Gain(S, A) = E(S) - AE(S, A)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
