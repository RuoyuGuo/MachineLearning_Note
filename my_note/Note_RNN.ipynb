{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network(RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what is RNN\n",
    "\n",
    "RNN is a NN with memory. These memory could store information from previous input(need input to be sequential... like text, audio...)\n",
    "\n",
    "### Why RNN helps\n",
    "\n",
    "It can take semantics into consideration and use context to analyse target. \n",
    "\n",
    "For example, we have two phrase 'arrive Sydney' and 'leave Sydney'. In these two string, Sydney plays different roles 'destination' and 'point of departure' and these two roles are decided by 'arrive' and 'leave' rather than Sydney...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Structure of RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN(Elman Network)\n",
    "\n",
    "**intro**\n",
    "\n",
    "A simple RNN just add some memory cell for nueron to NN.\n",
    "\n",
    "Recall output of a nueron in NN = $\\sigma(W^T \\cdot X)$...\n",
    "\n",
    "The output of nueron = $\\sigma(W^T \\cdot X +  C)$, where C is data from corresponding cell.\n",
    "\n",
    "At the same time, data in cell will be set to $\\sigma(W^T \\cdot X +  C)$.\n",
    "\n",
    "**Example**\n",
    "\n",
    "Let's think a simple NN\n",
    "\n",
    "input: $\\begin{bmatrix}x^1 \\\\ x^2 \\end{bmatrix}$\n",
    "\n",
    "Two nueron: $w_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ and $w_2 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$, these 1s in here are weight\n",
    "\n",
    "weight of cell to input: $W' = 1$\n",
    "\n",
    "Two memory cell: $c_1 =  0$ and  $c_2 =  0$, the first cell for the first nueron, the 2nd for the 2nd nueron.\n",
    "\n",
    "Activation function: $y = x$\n",
    "\n",
    "output: $\\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix}$\n",
    "\n",
    "if our first input is \n",
    "$$\n",
    "x_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "Moreover, by definition of RNN\n",
    "$$\n",
    "y_1 = \\sigma(w_1^T \\cdot (x_1) +  C) =  \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}  + 0 = 2\n",
    "$$\n",
    "\n",
    "similarly, $y_2 = 2$.\n",
    "\n",
    "Then, $y_1$ and $y_2$ will be stored into $c_1$ and $c_2$, so we set $c_1 = 2$ and $c_2 = 2$.\n",
    "\n",
    "our second input is \n",
    "$$\n",
    "x_2 = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "By definition of RNN\n",
    "$$\n",
    "y_1 = \\sigma(w_1^T \\cdot (x_1) +  C) =  \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}  + 2 =  7\n",
    "$$\n",
    "\n",
    "similarly, $y_2 = 7$.\n",
    "\n",
    "Then, $y_1$ and $y_2$ will be stored into $c_1$ and $c_2$, so we set $c_1 = 7$ and $c_2 = 7$.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "This exmample shows that the output of nueron on a fixed RNN not only depends on input but also the content in memory cell which will be influenced by previous input.\n",
    "\n",
    "Besides, the memory cell can have weight, hence the output of nueron will be modified to \n",
    "$$\n",
    "\\sigma(W^T \\cdot X + W' \\cdot C)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jordan Network\n",
    "\n",
    "Cell doesn't store the output of nueron but the output of output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNN\n",
    "\n",
    "RNN will read the input firstly, then read it again in reverse direction...\n",
    "\n",
    "Then give output, these could take the whole content not only previous content into consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short-term Memory(LSTM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intro**\n",
    "\n",
    "Each nueron in NN will be replaced by a special nueron which is LSTM in here with 4 input and 1 output.\n",
    "\n",
    "**structure of sepcial nueron(LSTM)**\n",
    "\n",
    "$W$ of input. \n",
    "\n",
    "subscript $_t$ means time steps.\n",
    "\n",
    "There are connections between output and input of a LSTM which is $h$ in here. $h_{t-1}$ is previous output of LSTM. $U$ is the weight of it. This tells us that a nueron not only depends on input x but also last output.\n",
    "\n",
    "New input Data:\n",
    "* $y_d = \\sigma(W_d \\cdot x_t + U_d \\cdot h_{t-1} + b_d)$\n",
    "* we need to do some transformation to our input data...\n",
    "\n",
    "Input Gate:\n",
    "* $y_i = \\sigma(W_i \\cdot x_t + U_i \\cdot h_{t-1} + b_i)$\n",
    "* It controls the extent of new input data. Technically, the weight of new input data.\n",
    "\n",
    "Forget Gate:\n",
    "* $y_f = \\sigma(W_f \\cdot x_t + U_f \\cdot h_{t-1} + b_f)$\n",
    "* It controls what the extent of content in memory cell will be kept to next time step.\n",
    "\n",
    "Memory cell:\n",
    "* $y_c = c \\cdot y_f + y_i \\cdot y_d$\n",
    "* c is the current content in memory cell. \n",
    "\n",
    "output of Memory cell:\n",
    "* $y_m = \\sigma(y_c)$\n",
    "* need to go across an activation function\n",
    "\n",
    "Output Gate:\n",
    "* $y_o = \\sigma(W_o \\cdot x_t + U_o \\cdot h_{t-1} + b_o)$\n",
    "* It controls the extent of output . Technically, the weight of output.\n",
    "\n",
    "$y_o$ is the final output of a special nueron.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "To get the output of a special nueron, we need to do all computation in above.\n",
    "\n",
    "Sometimes, we don't directly use $x_t$ as input in any Gate or New input data. $x_t$ will pass specific transformation and becomes $x_d$ or $x_i$ etc... Them will replace $x_t$ in each formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with peephole\n",
    "\n",
    "we need to do some little change to LSTM. Firstly, we use $C_{t-1} = [c_1, c_2, c_3...]$ to represent the content in memory cell in this layers, where $c_1$ is the content in memory cell in first nueron.\n",
    "\n",
    "Intuitively, input of any gate should alse take $C_{t-1}$ into consideration.\n",
    "\n",
    "Now, new formula are showing below, we use $V$ to represent the weight of cell $C$.\n",
    "\n",
    "New input Data:\n",
    "* $y_d = \\sigma(W_d \\cdot x_t + U_d \\cdot h_{t-1}  + b_d)$\n",
    "\n",
    "Input Gate:\n",
    "* $y_i = \\sigma(W_i \\cdot x_t + U_i \\cdot h_{t-1} + V_i \\cdot C_{t-1} + b_i)$\n",
    "\n",
    "Forget Gate:\n",
    "* $y_f = \\sigma(W_f \\cdot x_t + U_f \\cdot h_{t-1}+ V_f \\cdot C_{t-1}  + b_f)$\n",
    "\n",
    "Memory cell:\n",
    "* $y_c = c \\cdot y_f + y_i \\cdot y_d$\n",
    "\n",
    "output of Memory cell:\n",
    "* $y_m = \\sigma(y_c)$\n",
    "\n",
    "Output Gate:\n",
    "* $y_o = \\sigma(W_o \\cdot x_t + U_o \\cdot h_{t-1} + V_o \\cdot C_{t-1} + b_o)$\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "To get the output of a special nueron, we need to do all computation in above.\n",
    "\n",
    "Sometimes, we don't directly use $x_t$ as input in any Gate or New input data. $x_t$ will pass specific transformation and becomes $x_d$ or $x_i$ etc... Them will replace $x_t$ in each formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Important issue in LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### performance on LSTM\n",
    "\n",
    "Not very well, sometimes the Loss doesn't gradually reduce epoch by epoch. Otherwise, it change rapidly and uneven.\n",
    "\n",
    "### Why (example)?\n",
    "\n",
    "Go back to our simplest RNN, $y_t = (1 \\cdot x_t + c_{t-1})$, $c_t =  x_t + w \\cdot c_{t-1}$\n",
    "\n",
    "Let's look an example. \n",
    "\n",
    "when we use linear function as our activation function and first input is 1. The remaining input are 0.\n",
    "\n",
    "$y_1 = 1 \\cdot 1 + w \\cdot 0 = 1$, $y_2 = 1 \\cdot 0 + w \\cdot 1 = w $ and so on.\n",
    "\n",
    "we can induce that $y_{1000} = w^{999}$.\n",
    "\n",
    "Exponentiation tell us even little change in w, the reulst could change signicantly. It might be extremely large or extremely small.\n",
    "\n",
    "This is why $\\frac{\\partial L}{\\partial W}$ change unsmoothly.\n",
    "\n",
    "Therefore, gradient vanish problem hardly happen in LSTM but not gradient explode.\n",
    "\n",
    "In real training, when gradient increase, learning rate increase, gradient decrease, learning rate decrease.\n",
    "\n",
    "We cannot get a good performance in RNN because this exmaple status that gradient change significantly sometimes in RNN. \n",
    "\n",
    "### Why (thoery)?\n",
    "\n",
    "In LSTM, Memory cell store both input value and its original value. Any influence to these value doesn't disappear until forget gate is closed($y_f = 0$).  Unfortunnately, LSTM is invented to solve gradient vanish problem hence bias will be initialised to make forget gate open occasionally.\n",
    "\n",
    "**Why simple RNN doesn't have similar issue?**\n",
    "\n",
    "recall that the update of value in memory cell in simple RNN is completely replcaed by the output of nueron.\n",
    "\n",
    "\n",
    "### How to solve?\n",
    "\n",
    "replcae LSTM by Gated Recurrent Unit(GRU)-- a simple version of LSTM. \n",
    "\n",
    "Set A upper bound for gradient..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many and one are length of input or output.\n",
    "\n",
    "### Many to one (input: many, output: one)\n",
    "\n",
    "Sentiment analyse...\n",
    "\n",
    "### Many to one\n",
    "\n",
    "Key Term Extraction...\n",
    "\n",
    "### Many to Many(output is still shorter than input)\n",
    "\n",
    "speech recognition\n",
    "\n",
    "### Many to Many(output and input has no limitation)\n",
    "\n",
    "Machine Translation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
