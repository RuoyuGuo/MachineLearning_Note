{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, ensemble is trying to train a number of models on same task but has different data. The combination of these models may usually have a better performance than single model.\n",
    "\n",
    "We will introduce 3 ensemble method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to train\n",
    "\n",
    "First of all, Sampling $t$(decide by you) sub-dataset from whole dataset($N$ data)and each dataset has each dataset has $N'$(usually, $N' == N$) examples. Don't worry that sub-dataset may have the same data with original dataset because it may sample the same example.\n",
    "\n",
    "Next, we use these $t$ sub-dataset to training $t$ models(same model but has different parameters).\n",
    "\n",
    "Get final model by averaging these models.\n",
    "\n",
    "### How to do validation\n",
    "\n",
    "Out-of-bag(OOB) validation: Some examples in orignial dataset may be not in some sub-dataset. We could use those data not in sub-dataset to test model.\n",
    "\n",
    "### Tips\n",
    "\n",
    "To different models, resampling training data is not sufficient. It might need collaboration of some other ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Boosting?\n",
    "Boosting is an ensemble method to improve model performance. Sound unbelievable, However, you can indeed obtain 0% error rate after boosting. If you want to use boosting. Your model must have error rate less than 50%. We will explain why error rate must less than 50% later. \n",
    "\n",
    "### General process:\n",
    "* we discuss classification problem in here\n",
    "* give a weak classifier $f_1(x)$ as first classifier\n",
    "* get the next classifier $f_2(x)$ that can be complementary with $f_1(x)$\n",
    "* get the third classifier $f_3(x)$ that can be complementary with $f_2(x)$\n",
    "* ...\n",
    "* aggregate all classifier\n",
    "* we learn classifier sequentially\n",
    "\n",
    "### How to get next classifier?\n",
    "* training on different training data sets can obtain different classifiers\n",
    "* Now the problem is how to obtain different training data sets?\n",
    "    * resampling your data, like what we did in Bagging.\n",
    "    * reweighting your data\n",
    "    \n",
    "        For example, if you have $(x^1, \\hat{y}^1, u^1 = 1), (x^2, \\hat{y}^2, u^2 = 1), (x^3, \\hat{y}^3, u^3 = 1)$, where x is features and y is label, we can change data by changing $u_i$. The real data using in training will be mutiplied by corresponding $u$. That is $x_i = x_i \\cdot u_i$, $\\hat{y}^i = \\hat{y}^i \\cdot u_i$. \n",
    "\n",
    "        Additionally, loss function should be modified to $L(f) = \\sum_n u^n l(f(x^n), \\hat{y}^n)$ (orignial: $L(f) = \\sum_nl(f(x^n), \\hat{y}^n)$), a weighted version of loss function.\n",
    "        \n",
    "**Now, let's introduce some algorithms to do boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Algorithm: Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation\n",
    "\n",
    "Supposing we have $f_1(x)$ with error rate $\\epsilon_1$ less than $0.5$ on orignial training data sets(with weight 1). \n",
    "\n",
    "### How to calculate error rate on weighted data sets?\n",
    "$$\n",
    "\\epsilon_1 = \\frac{\\sum_n u_1^n \\delta(f_1(x^n) \\ne \\hat{y}^n)}{Z_1}, Z_1 = \\sum_n u_1^n, \\epsilon_1 < 0.5\n",
    "$$\n",
    "where $Z_1$ is summantion of weight on dataset $X_1$, $u_i^n$ is the weight of n'th data on i'th iteration.\n",
    "\n",
    "### What kind of reweighted data we want to obtain?\n",
    "\n",
    "We want to reweight data such that $f_1(x)$ will fail to classify reweighted dataset(random classification). Since we use binary classification as example, random classification on such data will obtain $50\\%$ error rate.\n",
    "\n",
    "Thinking about the formula of calculation of error rate,  the error rate will rise if we increase the weight on misclassified data and decrease the weight on correctly clasified data.\n",
    "\n",
    "### How to reweight training data?\n",
    "\n",
    "* If $x^n$ misclassified by $f_1$($f_1(x^n) \\ne \\hat{y}^n$)\n",
    "   \n",
    "    new $u_{i+1}^n = u_i^n \\cdot d_i$\n",
    "  \n",
    "* If $x^n$ correctly classified by $f_1$($f_1(x^n) == \\hat{y}^n$)\n",
    "\n",
    "    new $u_{i+1}^n = u_i^n / d_i$\n",
    "\n",
    "### How to compute $d_i$\n",
    "\n",
    "Go back to the formula of calculating error rate. Recall that we are talking about binary classificatoin.\n",
    "\n",
    "We know \n",
    "\n",
    "$$\n",
    "\\epsilon_1 = \\frac{\\sum_n u_1^n \\delta(f_1(x^n) \\ne \\hat{y}^n)}{Z_1}, Z_1 = \\sum_n u_1^n, \\epsilon_1 < 0.5\n",
    "$$\n",
    "\n",
    "and we want $f_1$ get $50\\%$ error rate on new dataset, that is\n",
    "\n",
    "$$\n",
    "\\epsilon_1' = \\frac{\\sum_n u_2^n \\delta(f_1(x^n) \\ne \\hat{y}^n)}{Z_2} = 0.5\n",
    "$$\n",
    "\n",
    "$Z_i$ is the summation of weight. It can be splitted to summation of weight of misclassified data plus summation of weight of correctly classified data\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_n u_2^n \\delta(f_1(x^n) \\ne \\hat{y}^n)}{Z_2} = \n",
    "\\frac{\\sum_n u_2^n \\delta(f_1(x^n) \\ne \\hat{y}^n)}                    \n",
    "     {\\displaystyle\\sum_{f_1(x^n) = \\hat{y}^n}u_2^n + \\displaystyle\\sum_{f_1(x^n) \\ne \\hat{y}^n}u_2^n} \n",
    "$$\n",
    "\n",
    "recall that how we reweight data by $d$,\n",
    "\n",
    "$$\n",
    "=\n",
    "\\frac{\\sum_n u_1^n \\cdot d_1 \\delta(f_1(x^n) \\ne \\hat{y}^n)}\n",
    "{\\displaystyle\\sum_{f_1(x^n) = \\hat{y}^n}u_1^n / d_1 + \\displaystyle\\sum_{f_1(x^n) \\ne \\hat{y}^n}u_1^n \\cdot d_1}\n",
    "$$\n",
    "\n",
    "reverse $\\epsilon_1'$, \n",
    "\n",
    "$$\n",
    "\\frac{1}{\\epsilon_1'} = \\frac\n",
    "{\\displaystyle\\sum_{f_1(x^n) = \\hat{y}^n}u_1^n / d_1 + \\displaystyle\\sum_{f_1(x^n) \\ne \\hat{y}^n}u_1^n \\cdot d_1}\n",
    "{\\sum_n u_1^n \\cdot d_1 \\delta(f_1(x^n) \\ne \\hat{y}^n)}\n",
    "= 2\n",
    "$$\n",
    "\n",
    "then,\n",
    "$$\n",
    "= \n",
    "\\frac\n",
    "{\\displaystyle\\sum_{f_1(x^n) = \\hat{y}^n}u_1^n / d_1}\n",
    "{\\sum_n u_1^n \\cdot d_1 \\delta(f_1(x^n) \\ne \\hat{y}^n)} \n",
    "+ \n",
    "\\frac\n",
    "{\\displaystyle\\sum_{f_1(x^n) \\ne \\hat{y}^n}u_1^n \\cdot d_1}\n",
    "{\\sum_n u_1^n \\cdot d_1 \\delta(f_1(x^n) \\ne \\hat{y}^n)} \n",
    "= \n",
    "\\frac\n",
    "{\\displaystyle\\sum_{f_1(x^n) = \\hat{y}^n}u_1^n / d_1}\n",
    "{\\sum_n u_1^n \\cdot d_1 \\delta(f_1(x^n) \\ne \\hat{y}^n)} \n",
    "+\n",
    "1\n",
    "$$\n",
    "\n",
    "since $\\delta(...)$ return either $0$ or $1$\n",
    "$$\n",
    "= \\frac\n",
    "{\\displaystyle\\sum_{f_1(x^n) = \\hat{y}^n}u_1^n / d_1}\n",
    "{\\displaystyle\\sum_{f_1(x^n) \\ne \\hat{y}^n} u_1^n \\cdot d_1 } \n",
    "+\n",
    "1\n",
    "=\n",
    "2\n",
    "$$\n",
    "\n",
    "Now, we get\n",
    "\n",
    "$$\n",
    "\\frac\n",
    "{\\displaystyle\\sum_{f_1(x^n) = \\hat{y}^n}u_1^n / d_1}\n",
    "{\\displaystyle\\sum_{f_1(x^n) \\ne \\hat{y}^n} u_1^n \\cdot d_1 } \n",
    "=\n",
    "1 \n",
    "\\rightarrow\n",
    "\\displaystyle\\sum_{f_1(x^n) = \\hat{y}^n}u_1^n / d_1\n",
    "=\n",
    "\\displaystyle\\sum_{f_1(x^n) \\ne \\hat{y}^n} u_1^n \\cdot d_1\n",
    "\\rightarrow\n",
    "\\frac{1}{d_1} \\cdot \\displaystyle\\sum_{f_1(x^n) = \\hat{y}^n}u_1^n  \n",
    "=\n",
    "d_1 \\cdot \\displaystyle\\sum_{f_1(x^n) \\ne \\hat{y}^n} u_1^n \n",
    "$$\n",
    "\n",
    "keep going, recall the calculation of $\\epsilon_1$, error rate is actually the ratio between summation of misclassified weight and total weight\n",
    "\n",
    "$$\n",
    "\\rightarrow\n",
    "d_1^2 = \n",
    "\\frac{\\displaystyle\\sum_{f_1(x^n) = \\hat{y}^n}u_1^n}\n",
    "{\\displaystyle\\sum_{f_1(x^n) \\ne \\hat{y}^n} u_1^n}\n",
    "= \n",
    "\\frac{(1 - \\epsilon_1)\\cdot Z_1}\n",
    "{\\epsilon_1 \\cdot Z_1}\n",
    "=\n",
    "\\frac{(1 - \\epsilon_1)}\n",
    "{\\epsilon_1}\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "d_i =\n",
    "\\sqrt\n",
    "\\frac{1 - \\epsilon_i}\n",
    "{\\epsilon_i} > 1\n",
    "$$\n",
    "\n",
    "remember we want to $\\epsilon < 0.5$, so $d_i > 1$. We want to increase weight of misclassified data by multiplying $d_i$. This is exactly what we get in here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion of Adaboost\n",
    "\n",
    "* Given training data and initial weight $\\{(x^1, \\hat{y}^1, u_1^1), ..., (x^k, \\hat{y}^k, u_1^k), ..., (x^n, \\hat{y}^n, u_1^n)\\}$, initial weight is 1.\n",
    "* Repeat t = 1, ..., T:\n",
    "    * training weak classifier $f_t(x)$ with weights {u_t^1, ..., u_t^n}\n",
    "    * compute $\\epsilon_t = \\frac{\\sum_{f_t(x) \\ne \\hat{y}} u_t^n }{Z_t}$ \n",
    "    * update weight:\n",
    "        * introduce $\\alpha_t = \\ln{d_t} = \\ln{\\sqrt{(1 - \\epsilon_t)/\\epsilon_t}}$\n",
    "        * if $f_t(x) \\ne \\hat{y}$, then $u_{t+1}^n = u_t^n \\cdot d_t = u_t^n \\cdot \\exp(\\alpha_t)$\n",
    "        * if $f_t(x) = \\hat{y}$, then $u_{t+1}^n = u_t^n \\cdot d_t = u_t^n \\cdot \\exp(-\\alpha_t)$\n",
    "* final classifier(aggregate function), we use $H(x)$ to denote the aggregation results, \n",
    "    * uniform distribution, $H(x) = sign(\\sum^T_{t=1} f_t(x))$, if $H(x) > 0$, class 1, if $H(x) < 0$, class 0.\n",
    "    * This doesn't make sense...\n",
    "    * Weighted aggretation, $H(x) = sign(\\sum^T_{t=1} \\alpha_t f_t(x))$, we use this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why error rate down to 0 on training dataset?\n",
    "\n",
    "Final classifier: $H(x) = sign(\\sum^T_{t=1} \\alpha_t f_t(x))$\n",
    "\n",
    "then final error rate is \n",
    "\n",
    "$$\n",
    "\\frac{1}{N} \\sum_n \\delta(H(x^n) \\ne \\hat{y}^n) \n",
    "=^{(1)}\n",
    "\\frac{1}{N} \\sum_n \\delta(\\hat{y}^n g(x^n) < 0)\n",
    "\\le^{(2)}\n",
    "\\frac{1}{N} \\sum_n \\exp(-\\hat{y}^n g(x^n))\n",
    "=^{(3)}\n",
    "\\frac{1}{N}Z_{T+1}\n",
    "\\le^{(4)}\n",
    "\\prod_{t=1}^T 2\\sqrt{\\epsilon_t(1 - \\epsilon_t)}\n",
    "$$\n",
    "\n",
    "Since we guarantee $\\epsilon_t < 0.5$, final error rate converge to 0.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### break down\n",
    "\n",
    "**(1)**: We set $g(x^n) = \\sum^T_{t=1} \\alpha_t f_t(x^n)$, the left side on (1) is to compute the part of incorrect predicted results. These incorrect results are $H(x^n) \\ne \\hat{y}^n$ which are actually those $g(x^n)$ and $\\hat{y}^n$ not in same region(positive and negative, or negative and positive). \n",
    "\n",
    "**(2)**: $\\exp(...)$ is the upper bound of $\\delta(...)$. This can be verified by drawing plots.\n",
    "\n",
    "**(3)**: $Z_{T+1} = \\sum_n u_{T+1}^n$. \n",
    "* We know $u_1^n = 1$\n",
    "* and $u_{t+1}^n = u_t^n \\cdot \\exp(-\\hat{y}^n f_t(x^n) \\alpha_t)$. \n",
    "* By induction, $u^n_{T+1} = \\prod_{t=1}^T\\exp(-\\hat{y}^n f_t(x^n) \\alpha_t)$. \n",
    "* Therefore $Z_{T+1} = \\sum_n \\prod_{t=1}^T\\exp(-\\hat{y}^n f_t(x^n) \\alpha_t) = \\sum_n\\exp(-\\hat{y}^n \\sum_{t=1}^T \\alpha_t f_t(x^n) ) = \\sum_n\\exp(-\\hat{y}^n g(x^n))$\n",
    "\n",
    "**(4)**: recall $\\epsilon_t = \\frac{\\sum_{f_t(x^n) \\ne \\hat{y^n}} u^n}{Z_t}$\n",
    "* $Z_1 = N$\n",
    "* $Z_{t+1} = Z_t \\cdot \\epsilon_t \\cdot \\exp(\\alpha_t) + Z_{t-1} (1 - \\epsilon_t) \\exp (- \\alpha_t)$\n",
    "* = $Z_t \\cdot \\epsilon_t \\cdot \\sqrt((1-\\epsilon_t)/\\epsilon_t) + Z_t \\cdot (1-\\epsilon_t) \\cdot \\sqrt((1-\\epsilon_t)/\\epsilon_t) $\n",
    "* = $Z_t \\cdot 2 \\cdot \\sqrt(\\epsilon_t (1 - \\epsilon_t))$\n",
    "* Therefore, $Z_{T+1} = N \\prod_{t=1}^T 2 \\cdot \\sqrt{\\epsilon_t(1 - \\epsilon_t)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test error rate decreases even training error rate down to 0\n",
    "\n",
    "error rate($\\frac{1}{N} \\sum_n \\exp(-\\hat{y}^n g(x^n))$) tells us machine try to find $g(x^n)$ such that error rate as less as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting(general version)\n",
    "\n",
    "... It is adaboost actually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Suppose we already have several models to predict.\n",
    "\n",
    "We use the output of these results as input to train a final model to do the same task.\n",
    "\n",
    "Like a NN model.\n",
    "\n",
    "However, you need to split training data into two parts, first part to train several models, another one for final model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
