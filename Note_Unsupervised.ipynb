{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>by Clustering (linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-means**\n",
    "\n",
    "1. suppose we have an unlabelled dataset X\n",
    "\n",
    "2. define a K by your own, where k is the number of cluster(class)\n",
    "\n",
    "3. initialise cluster center $C = \\{c^1, c^2, ..., c^k\\}$, they are better collectted by random sampling k data from X\n",
    "\n",
    "4. repeat:\n",
    "    * for all $x^n \\in X$, find the cluster is the most close to $x^n$, then put it in to this cluster\n",
    "    * update cluster center by compute the average the $x^n$ that belong to this cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hierarchical Agglomerative Clustering(HAC)**\n",
    "\n",
    "1. suppose we have an unlabelled dataset X\n",
    "2. build a tree structure by repeat the following steps:\n",
    "    * see all data as nodes\n",
    "    * find two most close data (vector)\n",
    "    * build a parent of them\n",
    "    * find two most close data from all nodes without parents\n",
    "    * build a parent of them\n",
    "3. until we have a root node to all nodes\n",
    "4. pick a threshold, then the children of the threshold is the number of class, and descendant of a child node belong to same class  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>by Dimension Reduction (linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need find a function which the input and output both are data features. However, the dims of output is less than intput."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature selection**\n",
    "\n",
    "Analyse your data and reduce dims..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principle component analysis(PCA)**\n",
    "\n",
    "When we reduce dimension of features, it is nothing more than find a matrix $W$, such that, $z = Wx$, where x is our originial data and z is the data modified. In other words, that is to project our data to new dimensions. Moreover, the modified data must also be distinguished, which means you have to let $Var[x]$ as large as possible.\n",
    "\n",
    "How many dimensions that we want to keep eventually? This is defined by $W$, that is if $W$ has $x$ rows, then results will have $x$ dimensions.\n",
    "\n",
    "*Theory and property*:\n",
    "\n",
    "*orthogonal matrix*: $W^T = W^{-1} \\Leftrightarrow W^TW = WW^T = I $,\n",
    "\n",
    "*eigenvector*: for a square matrix $A$, the eigenvector $v$ and eigenvalue $\\lambda$ of $A$ make the equation true:\n",
    "\n",
    "$$\n",
    "    Av = \\lambda v\n",
    "$$\n",
    "\n",
    "if $w^n$ is a vector,\n",
    "\n",
    "then $w = \\begin{bmatrix}\n",
    "       (w^1)^T  \\\\[0.3em]\n",
    "       (w^2)^T \\\\[0.3em]\n",
    "        ...\n",
    "     \\end{bmatrix}$ is a orthogonal matrix\n",
    "     \n",
    "Let $S = Cov(x) = \\sum(x - \\bar{x})(x - \\bar{x})^T$\n",
    "\n",
    "NB: each data is a row in x\n",
    "\n",
    "$w^1$ is the eignevector of covariance matrix $S^T$ Corresponding to the largest eigenvalue $\\lambda$.\n",
    "\n",
    "$w^n$ is the n-th largest...\n",
    "\n",
    "In PCA $w^n$ are called principle axes\n",
    "\n",
    "eigenvalue are called principle component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA-Another Point of View**\n",
    "\n",
    "Any sample in your data could be seen as a weighted combination of some components. \n",
    "\n",
    "if we illustrate it in equation, that is \n",
    "\n",
    "$$\n",
    "    x - \\bar x \\approx c_1u^1 + c_2u^2 + ... + c_ku^k = \\hat{x}\n",
    "$$\n",
    "\n",
    "in this equation, $x$ is our data, $u^k$ is component, and $c_k$ is weight.\n",
    "\n",
    "Obviously, if we can find proper components, then data $x$ could be represented by \n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "    c_1 \\\\\n",
    "    c_2 \\\\\n",
    "    ... \\\\\n",
    "    c_k\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now the question is how to find the them.\n",
    "\n",
    "$x - \\bar x$ is constant in a data set.  Apparently, these components should minimise the distance between $(x - \\bar x)$ and $\\hat{x}$. \n",
    "\n",
    "let's to see $|| (x - \\bar x -) - \\hat x || _2$ as reconstruction error, \n",
    "\n",
    "our job is to find $\\{u^1, ..., u^k\\}$ minimise the error.\n",
    "\n",
    "if we use equation to illustrate it, that is \n",
    "\n",
    "$$\n",
    "    L = min \\sum ||(x-\\bar x) - (\\sum c_ku^k) ||_2\n",
    "$$\n",
    "\n",
    "In PCA, $\\{w^1, w^2, ..., w^k\\}$ is the component $\\{u^1, ..., u^k\\}$.\n",
    "\n",
    "**How to find $c^k$**\n",
    "\n",
    "$c^k = (x - \\bar x) \\cdot w^k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA tips\n",
    "\n",
    "**Standardization**:\n",
    "\n",
    "for those variables range and units are different such as you have temperature measured in degrees Celsius and rainfall measured in cm.\n",
    "\n",
    "It is good to centering the variable by subtracting mean(lead mean to 0) \n",
    "\n",
    "if the importance of features is independent of the variance of features, then divide them by standard deviation(lead variance to 1)\n",
    "\n",
    "**reconstruction:**\n",
    "\n",
    "since some eigenvector is negative, do\n",
    "* M -= np.min(M)\n",
    "* M /= np.max(M)\n",
    "* M = (M * 255).astype(np.unit8)\n",
    "\n",
    "to deliminate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA and SVD\n",
    "\n",
    "reference: https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what is it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse the hidden features behind data\n",
    "\n",
    "Let's suppose we have a matrix, while the each row represent a person, and columns different goods. \n",
    "\n",
    "$n_{ij}$ is the number of commodity j purchased by person i.\n",
    "\n",
    "There are some latent factors decide $n_{ij}$. For example, Some person may more like electronic device or toys. Similarly, a commodity could also be a electronic device or toy. \n",
    "\n",
    "Hence, we could find unique vector for each person $r_i$ = $\\{a_i, b_i...\\}$ and vecotr for each commodity $c_j$ = $\\{a_j, b_j...\\}$. The product of $r_i$ and $c_i$ decide how many js might be purchased by i.\n",
    "\n",
    "SVD actually achieve matrix factorization. $M = U\\Sigma V$, the only thing we need to do is decide $U \\cdot \\Sigma$ is person latent vector and $V$ is commodity latent vector or $U$ is person latent vecotr and $\\Sigma \\cdot V$ is commodity latent vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implementation\n",
    "\n",
    "use gradient descent to minimise:\n",
    "\n",
    "$$\n",
    "    L = \\sum_{i, j} (r^i \\cdot c*j - n_{ij})^2\n",
    "$$,\n",
    "\n",
    "some cell might be unknown value, hence ignore these cell when we implement gradient descent. Then we could predict these missing value, this is recommendation system.\n",
    "\n",
    "advanced matrix factorization:\n",
    "\n",
    "$$\n",
    "    L = \\sum_{i, j} (r^i \\cdot c*j + b_i + b_j - n_{ij})^2\n",
    "$$, where $b_i$ and $b_j$ are some outer reason that influence customer decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### application\n",
    "\n",
    "recommendation system, Topic analysis(LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> for Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encode word to vector, or use a vector to represent word.\n",
    "\n",
    "The dimensions of vector should be quite less than the total number of word.\n",
    "\n",
    "Machine learn the meaning of words from reading a lot of documents without supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to exploit the context?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count based**\n",
    "\n",
    "if two words $w_i$ and $w_j$ frequntly co-occur, $V(w_i)$ and $V(w_j)$ would be close to each other\n",
    "\n",
    "$V(w_i) \\cdot V(w_j) = N_{i,j}$ Number of times $w_i$ and $w_j$ in the same document\n",
    "\n",
    "**prediction based**\n",
    "\n",
    "input:1-of-N encoding $[0,...,1,...,0]$ of word $w_{i-1}$\n",
    "\n",
    "output: the probablity of each word as the next word $w_i$\n",
    "\n",
    "the first hidden layer is the word embedding vector\n",
    "\n",
    "**prediction-based-sharing parameters**\n",
    "\n",
    "use more words $w_{i-n}w_{i-n-1}...w_{i-1}$ to predict $w_i$\n",
    "\n",
    "$w_{i-n}...w_{i-1}$ has the same weight in the first hidden layer.\n",
    "\n",
    "let's say $z$ is the output of the first hidden layer, $x_i$ reprsent vecotr of word, $w_i$ represent weigth. In here, $z$ is the word embedding vector\n",
    "\n",
    "Then $z = W_{i-1}X_{i-1} + W_{i-2}X_{i-2} = W(X_{i-1} + X_{i-2})$ \n",
    "\n",
    "To make $w_{i-1} = w_{i-2}$, the update function should be \n",
    "$$\n",
    "w_{i-1} \\leftarrow w_{i-1} - \\eta \\frac{\\partial C}{\\partial w_{i-1}} - \\eta \\frac{\\partial C}{\\partial w_{i-2}} \n",
    "$$\n",
    "\n",
    "$$\n",
    "w_{i-2} \\leftarrow w_{i-2} - \\eta \\frac{\\partial C}{\\partial w_{i-1}} - \\eta \\frac{\\partial C}{\\partial w_{i-2}} \n",
    "$$\n",
    "\n",
    "**prediction-based-Variou Architectures**\n",
    "\n",
    "* continuous bag of word(CBOW) model: use context words(words surrounding targets) to predict target word\n",
    "\n",
    "* Skip-gram: use word to predict context words(words surrounding the word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Embedding\n",
    "\n",
    "**bag of word**\n",
    "\n",
    "frequency of each word\n",
    "\n",
    "problem: same bag may have different meaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Neighbor Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maniflod Learning (Non-linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a high dimension space, Euclidean distance cannot represent the correct different between two data point.\n",
    "\n",
    "Hence dimension reduction should be used at first, then use normal unsupervised learning method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Locally Linear Embedding(LLE)**\n",
    "\n",
    "Use K neighbors to represent target data. \n",
    "\n",
    "Let $x_i$ be the data we want to use its neighbor to represent, $x_j$ be $x_i$'s neighbor and $w_{ij}$ represents the relation when we use j to represent i.\n",
    "\n",
    "Since we want to use $x_j$ to represent $x_i$.\n",
    "\n",
    "The sum of Loss could be easily illustrate in the following formula\n",
    "\n",
    "$$\n",
    "        \\sum_{i}||x^i - \\sum_{j}w_{ij}x^j||_2\n",
    "$$\n",
    "\n",
    "The theory behind LLE is whatever we change the dimension of $x_i$ and $x_j$, their relation is constant, the dimension reduction results should based on $w_{ij}$.\n",
    "\n",
    "That is $z_i = w_{ij}z_j$, where $z$ is the vector after implementing the dimension reduction.\n",
    "\n",
    "Therefore, our goal is to find a set of $z$ can minimise the Loss:\n",
    "\n",
    "$$\n",
    "        \\sum_{i}||z^i - \\sum_{j}w_{ij}z^j||_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Laplacian Eigenmaps(not clear)**\n",
    "\n",
    "recall the smoothness in simi-supervised Learning: if two data point are close in high density region, then their label have the high probablity to be same.\n",
    "\n",
    "Our Loss function is revised to \n",
    "\n",
    "$$\n",
    "    L = \\sum_{i} f(y^i, \\hat{y^i}) + \\lambda S,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "    S = \\frac{1}{2} \\sum_{i,j} w_{i,j}(y^i-y^j)^2 = y^TLy\n",
    "$$\n",
    "\n",
    "Hence, the dimension reduction data are also follow the rules.\n",
    "\n",
    "That is \n",
    "\n",
    "$$\n",
    "    S = \\frac{1}{2} \\sum_{i,j} z_{i,j}(z^i-z^j)^2\n",
    "$$\n",
    "\n",
    "whith constraint, if the dim of $z$ is $M$, then $Span\\{z^i, z^2,...z^N\\} = R^M$\n",
    "\n",
    "then do clustering\n",
    "\n",
    "Laplacian eigenmaps + clustering = spectral clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-distributed Stochastic Neighbor Embedding(t-SNE)\n",
    "\n",
    "### intro\n",
    "\n",
    "let say the similarity of two data point $x^i$ and $x^j$ is defined by $S(x^i, x^j)$, $z^i$ and $z^j$ are data transformed by dimension reduction. \n",
    "\n",
    "the distribution of similarity of $x^i$ and $x^j$ is \n",
    "\n",
    "$$\n",
    "    P(x^j|x^i) = \\frac{S(x^i, x^j)}{\\sum_{k \\ne i} S(x^i, x^k)}\n",
    "$$\n",
    "\n",
    "the distribution of similarity of $z^i$ and $z^j$ is \n",
    "\n",
    "$$\n",
    "    Q(z^j|z^i) = \\frac{S(z^i, z^j)}{\\sum_{k \\ne i} S(z^i, z^k)}\n",
    "$$\n",
    "\n",
    "They would have close distrubtion. How to measure the similarity between two distribution, greater result, less similar\n",
    "\n",
    "*[what is Kullback–Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)*\n",
    "\n",
    "### Loss function\n",
    "\n",
    "$$\n",
    "    L = \\sum_{i}KL(P(*|x^i)||Q(*|z^i)) = \\sum_{i}\\sum_{j}P(x^j|x^i)log\\frac{P(x^j|x^i)}{Q(z^j|z^i)}\n",
    "$$\n",
    "\n",
    "### similarity measure\n",
    "\n",
    "Many functions are usefull to our similarity measurement.\n",
    "\n",
    "We often use:\n",
    "* $S(x^i, x^j) = exp(-||x^i - x^j||_2)$\n",
    "* $S(z^i, z^j) = exp(-||z^i - z^j||_2)$ or $S(z^i, z^j) =  \\frac{1}{1 + ||z^i - z^j||_2}$\n",
    "\n",
    "The exponential version of $S(z^i, z^j)$ is so called SNE, while another one is t-SNE.\n",
    "\n",
    "the performance of t-SNE is better than SNE because it will reinforce the difference of two data point.\n",
    "\n",
    "That is if orignial two data point are close in a high dimension, then they are still close in a low dimension.\n",
    "\n",
    "if original two data point are litter different , then they are distinct in low dimension.\n",
    "\n",
    "### NB \n",
    "\n",
    "if you use t-SNE metho do unsupervised learning, the whole training process should be redone even one new data is added.\n",
    "\n",
    "to make the training more efficient, we normally use another method to reduce dimension like PCA then do t-SNE or use t-SNE to do visulisation(reduce to 2 dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Auto-encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what is it?\n",
    "\n",
    "**New training idea**\n",
    "\n",
    "We build two Neuron Network, encoder and decoder.\n",
    "\n",
    "--- | Encoder | Decoder\n",
    "---|---|---\n",
    "Input:| image(training) | a vector\n",
    "Output:| a vector  | image(generative)\n",
    "\n",
    "Encoder will output a vector to represent the input image and Decoder will use this vector as its input.\n",
    "\n",
    "The output of decoder is also a image but it is generated by machine.\n",
    "\n",
    "We train Encoder and Decoder together and the restriction is clear\n",
    "\n",
    "Let the generated image be similar to the input data as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "reference: http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/\n",
    "\n",
    "In autoencoder, we want to find a function $f$ such that $f(x) \\approx x$.\n",
    "\n",
    "It has a greate probability of overfitting because machine may use some way to store all features of your data then output. \n",
    "\n",
    "Hence, we could impose some constraints to our model:\n",
    "\n",
    "* L1 norm\n",
    "* code layer has less neurons\n",
    "* sparsity, constrain these neurons to be inactive most of the time.\n",
    "* sigmoid or tanh might be better than relu\n",
    "\n",
    "**How to impose sparsity constraint?**\n",
    "\n",
    "let $\\hat{p_j}$ be the density of active neurons in code layer.\n",
    "\n",
    "That is \n",
    "$$\n",
    "    \\hat{p_j} = \\frac{\\text{number of active neurons}}{\\text{number of all neurons}} \n",
    "$$\n",
    "\n",
    "we would like to encofre $\\hat{p_j} =  p$(say $p = 0.05$). In other words, we would like there are only 0.05% neurons in code layer actived.\n",
    "\n",
    "It should note that different activation function might have different definition of active and inactive, like relu, greater than 0 is active, 0 is inactive or in sigmoid, 1 is active, -1 is inactive.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Retrieval**\n",
    "\n",
    "Normal way to anaylse a document is Bag-of-word. The problem of Bag-of-word is that It only counts words and ignore semantics. \n",
    "\n",
    "Semantics could be taken into consideration if we use auto-encoder.\n",
    "\n",
    "Some project show that documents or querys with similar subject are close when encode them on 2 dim code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training-DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**intro**\n",
    "\n",
    "A good way to do initialisation for NN by Auto-encoder\n",
    "\n",
    "**implementation**\n",
    "\n",
    "Assuming hidden layer of your NN with shape $(1000, 1), (700, 1) (500, 1), (10, 1)$ and input is $(784, 1)$\n",
    "\n",
    "If you want to start with some proper parameters.\n",
    "\n",
    "Do\n",
    "* construct a auto-encoder with input: $(784, 1)$, one hidden layer $(1000, 1)$ (It's actually the first hidden layer in) and output $(784, 1)$\n",
    "\n",
    "* training your auto-encoder as normal way. That is to let output be close to input as much as possible.\n",
    "\n",
    "    * Be careful in here because when your hidden layer has more dimensions than your input it might output your original input and learn nothing\n",
    "    \n",
    "    * To avoid this you could use some strong regularization such as l1 in your auto-encoder\n",
    "    \n",
    "* add a new hidden layer $(700, 1)$ after $(1000, 1)$, then fix the parameters in $(1000, 1)$. Your output is $(1000, 1)$. \n",
    "\n",
    "* repeat them until you meet the code layer $(10, 1)$\n",
    "\n",
    "* Fine-tune: The parameters of $(10, 1)$ can be inialised randomly. After this, using backpropagation training your auto-encoder\n",
    "\n",
    "**Tips**\n",
    "\n",
    "Pre-training ever worked for large data but now it is not necessary.  \n",
    "\n",
    "There is one way to use it when you have large unlabelled dataset. Use it on your unlabelled dataset then tune your NN by labelled dataset.\n",
    "\n",
    "**De-noising auto-encoder**\n",
    "\n",
    "Adding noising value to your input data has little influence on code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-encoder for CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**intro**\n",
    "\n",
    "Suppose we are training a CNN with two convolution layers and two max pooling layers.\n",
    "\n",
    "We add two unpooling layer and two deconvolution layers.\n",
    "\n",
    "Then CNN becomes con->pool->con->pool->unpool->decon->unpool->decon\n",
    "\n",
    "**unpool**\n",
    "\n",
    "if the matrix of max pooling is $(2, 2)$, two ways to do unpool\n",
    "\n",
    "* remember entry of the max value, and unpool to previous matrix.  New entrys are filled with 0. That is \n",
    "$$\n",
    "    \\begin{matrix}\n",
    "        5 & 4 \\\\\n",
    "        3 & 1 \n",
    "    \\end{matrix}\n",
    "    ->pooling->    \n",
    "    \\begin{matrix}\n",
    "        5\n",
    "    \\end{matrix}\n",
    "    ->unpooling->\n",
    "    \\begin{matrix}\n",
    "        5 & 0 \\\\\n",
    "        0 & 0 \n",
    "    \\end{matrix}\n",
    "$$\n",
    "\n",
    "* or filled with same max value\n",
    "\n",
    "**Deconvolution**\n",
    "\n",
    "Assuming our data is $[x_1, x_2, x_3, x_4]$, filter is $[w_1, w_2, w_3]$ and stride is 1.\n",
    "\n",
    "When we do convolution, data becomes to $[[x_1, x_2, x_3] \\cdot W, [x_2, x_3, x_4] \\cdot W] = [a_1, a_2]$\n",
    "\n",
    "How to do Deconvolution?\n",
    "\n",
    "Set new unfilter to $[u_1, u_2, u_3]$\n",
    "\n",
    "data becomes to $[a_1 \\cdot u_1,  a_1 \\cdot u_2 + a_2 \\cdot u_1, a_1 \\cdot u_3 + a_2 \\cdot u_2, a_2 \\cdot u_3] = [x'_1, x'_2, x'_3, x'_4]$\n",
    "\n",
    "This process can be illustrated by a new fomular. Firstly, padding 0 to data.\n",
    "$$\n",
    "[0, 0, a_1, a_2, 0, 0]\n",
    "$$\n",
    "\n",
    "Then use new unflter $U$ do convolution.\n",
    "$$\n",
    "[[0, 0, a_1] \\cdot U, [0, a_1, a_2] \\cdot U, [a_1, a_2, 0] \\cdot U, [a_1, 0, 0] \\cdot U]\n",
    "$$\n",
    "\n",
    "It is exactly equal to $[x'_1, x'_2, x'_3, x'_4]$\n",
    "\n",
    "Therefore, Deconvolution is actually convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creationg - Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**intro**\n",
    "\n",
    "Let machine read a lot of image, then draw image by itself.\n",
    "\n",
    "**Generative Model**\n",
    "\n",
    "* PixelRNN\n",
    "\n",
    "* Variational Autoencoder(VAE)\n",
    "\n",
    "* Generatvie Adversarial Network(GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PixelRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training:** \n",
    "\n",
    "Reading a large amount of data.\n",
    "\n",
    "**Drawing**\n",
    "\n",
    "Randomly inialise a pixel, take it as input of Machine. The Machine will generate a pixel.\n",
    "\n",
    "Then take these two pixel as input. Machine will generate next pixel.\n",
    "\n",
    "Repeat until draw a image...\n",
    "\n",
    "Or take a part of known image, see what machine can draw.\n",
    "\n",
    "**More on it**\n",
    "can be used on audio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Autoencoder(VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**auto-encoder**\n",
    "\n",
    "input => NN encoder => code => NN decoder => output\n",
    "\n",
    "It cannot generate intermediate image that is similar to A and B.\n",
    "\n",
    "one to one model.\n",
    "\n",
    "**VAE**\n",
    "\n",
    "input => NN encoder \n",
    "\n",
    "=> $[m_1,...,m_n], [\\sigma_1, ..., \\sigma_n] \\text{ and } [e_1,..., e_n]$\n",
    "\n",
    "=> $[c_1, c_2, c_3]$ => NN decoder => output, \n",
    "\n",
    "where $c_i = exp(\\sigma_i) \\cdot e_i + m_i$\n",
    "\n",
    "We use the vector to do calculation. $\\sigma = [\\sigma_1, ..., \\sigma_n]...$\n",
    "\n",
    "the number of dimensions of $m, \\sigma, e$ depends on the dimension of code you want generate.\n",
    "\n",
    "$m$ and $\\sigma$ are generated by machine but $e$ is sampled from a normal distribution in $\\sigma$.\n",
    "\n",
    "**training**\n",
    "\n",
    "* Construction error: minimise (output should be closed to input, like in auto-encoder)\n",
    "\n",
    "* Error2: minimise $\\sum_i = exp(\\sigma_i) - (1 + \\sigma_i) + (m_i)^2$  $(m)^2$ is L2_Norm in here.\n",
    "\n",
    "**Intuitive reason**\n",
    "\n",
    "In this formular, $exp(\\sigma)$ represent the variation.\n",
    "\n",
    "We want some code within a range set that can be mapped to same output rather than a code to a output(Many to one model).\n",
    "\n",
    "This is why $\\sigma$ and $e$ are introduced in here. Moreover, if same code are overlap in two range set, then it might be generate intermediate image between two image.\n",
    "\n",
    "$\\sigma$ is learned by Machine, to avoid to get $\\sigma = 0$.\n",
    "\n",
    "Therefore, we introduce the Error2 in here. When $\\sigma \\approx 1$, we have the minimum $exp(\\sigma_i) - (1 + \\sigma_i)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gaussian Mixtrue Model**\n",
    "\n",
    "A complex distribution could be combined by several different Gaussian distribution.\n",
    "\n",
    "**Probability**\n",
    "\n",
    "Sample x from your dataset(from Gaussian Mixtrue Model).\n",
    "\n",
    "$P(x) = \\sum_m P(m)P(x|m)$, where $P(m)$ is the probablity you choose a *Gaussian distribution* from *Gaussian Mixtrue Model*.\n",
    "\n",
    "To know the *Gaussian distribution* generate x, we construct a NN, take x as input, output the mean and variance of *Gaussian distribtuion*.\n",
    "\n",
    "This is exactly decoder.\n",
    "\n",
    "Each x you generate is from a mixture Distributed representation is better than cluster.\n",
    "\n",
    "**where are x come from(In VAE)**\n",
    "\n",
    "Let us use z to represent a sample that you choose from dataset. It might be come from a Normal Distribution $(Z\\sim N(0, 1))$.\n",
    "\n",
    "Z is a vector to represent attributes of data.\n",
    "\n",
    "Now, $P(x) = \\int_z P(z) P(x|z)\\mathrm{d}x$.\n",
    "\n",
    "Construct a NN, input is z, output is the mean and variance of a *Gaussian distribution* where your z comes from from *Gaussian Mixtrue Model*(actually the previous P(x) distribution).\n",
    "\n",
    "The NN in here is exactly decoder.\n",
    "\n",
    "**Maximizing Likelihood**\n",
    "\n",
    "Similar to classification. Maximize $P(x)$\n",
    "\n",
    "Which is the second error we want to minimize.\n",
    "\n",
    "**final**\n",
    "\n",
    "We already have encoder and decoder. This is VAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Generatvie Adversarial Network(GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The evolution of generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**idea**\n",
    "\n",
    "Like the evolution of creatures. \n",
    "\n",
    "Animals will evoulute new atrributes to protect them from predatoring.\n",
    "\n",
    "With the evolution of animals, predators will also evolute new abilities. \n",
    "\n",
    "**come to NN**\n",
    "\n",
    "Construct a NN generator(V1) to generate our output.\n",
    "\n",
    "Construct a NN discriminator(V1) to evaluate the output(0 ~ 1, from fake to real).\n",
    "\n",
    "Tune your parameter, Construct a NN generator(V2) and a NN discriminator(V2)...\n",
    "\n",
    "**How first generation works**\n",
    "\n",
    "* Randomly sample a vector from a disribution.\n",
    "\n",
    "* input to NN Generator(V1)(it is a decoder in VAE) and output image x, label them to 0(fake)\n",
    "\n",
    "* label our training image to 1(real)\n",
    "\n",
    "* Train NN discriminator(V1) (binary classification)\n",
    "\n",
    "**How to training GAN(evolve to next generation)**\n",
    "\n",
    "Tune parameters in NN Generator V1(Gradient descent) such that the image generated by GV1 can have a good result on DV1.\n",
    "\n",
    "**Problem in GAN**\n",
    "\n",
    "reference: (https://www.youtube.com/watch?v=8zomhgKrsmQ&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&index=27)\n",
    "\n",
    "* GANs are difficult to optimize. \n",
    "* No explicit signal about how good the generator is \n",
    "    * In standard NNs, we monitor loss \n",
    "    * In GANs, we have to keep “well-matched in a contest”\n",
    "* When discriminator fails, it does not guarantee that generator generates realistic images \n",
    "    * Just because discriminator is stupid \n",
    "    * Sometimes generator find a specific example that can fail the discriminator\n",
    "    * Making discriminator more robust may be helpful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
