{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Introduction to transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what is it?\n",
    "\n",
    "Transfer learning is a training method that use 'source data' that is not directly relevant to your task to train your model.\n",
    "\n",
    "### type overview\n",
    "\n",
    "| - | - |source data | source data \n",
    "| :---: | :---: | :---: | :---: |\n",
    "| - | -  | labelled | unlabelld |\n",
    "| Target data| labelled | Model Fine-tuning|\n",
    "|Target data | unlabelled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label-label:\n",
    "\n",
    "* Target data:($x^t$, $y^t$) few\n",
    "* Source data:($x^s$, $y^s$) a large amount\n",
    "\n",
    "label-unlabelled:\n",
    "\n",
    "* Target data:($x^t$) as our testing data\n",
    "* Source data:($x^s$, $y^s$) as our training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Model Fine-tuning (label - label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "how to train:\n",
    "\n",
    "training your model using source data, then fine-tune the model on target data.\n",
    "\n",
    "Issue:\n",
    "\n",
    "Only has few target data, model might be easily overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type1: Conservative Training\n",
    "\n",
    "**intro**\n",
    "\n",
    "use parameters in the model trained by target data as initialisation. When you do fine-tune on your target data, add some constarint like \"for the same input, the output of two model(trained by target data and fine-tuned model) should be similar.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type2: Layer Transfer\n",
    "\n",
    "**intro**\n",
    "\n",
    "fix some layers in original model(keep the paramters) and only update or tune remaining layers.\n",
    "\n",
    "It can prevent overfitting since only update few parameters.\n",
    "\n",
    "You can fine-tune whole layers when have sufficient data.\n",
    "\n",
    "**which layers should be transferred(fixed)**\n",
    "\n",
    "Generally\n",
    "* Image: transfer the first few layers. (The first few layers normally recognise some global features)\n",
    "* Audio: transfer the last few layers. (The first few layers in a speech recognised NN often analyse the manner of articulation of speakers and remaining layers conclude the content.)\n",
    "\n",
    "**performance on image**\n",
    "\n",
    "some research shows that with more layers transferred, the performance on 'transfer + fine-tune' is higher than 'base line(only train model on target data), and 'base line' is higher than 'transfer'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Multitask Learning(label-label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### introduction\n",
    "\n",
    "Multitask Learning uses a multi-layer NN structure. \n",
    "\n",
    "That is, from some one hidden layer, it generates several branches hidden layers and these branches still connect their own layers. Each branch has their own output layer which complete different tasks.\n",
    "\n",
    "Or there are several input features and conneted to their own hidden layers, then they come to some common layers then seperate again.\n",
    "\n",
    "### Progressive Neural Networks\n",
    "\n",
    "construct a NN a to complete task 1.\n",
    "\n",
    "construct a NN b to complete task 2, but the input each layer in b is the output from previous layer in NN b and the output from a layer in a.\n",
    "\n",
    "construct a NN c to complete task3, but the input each layer in b is the output from previous layer in NN c, the output from a layer in b and the output from a layer in a.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Domain-adversarial training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### introduction\n",
    "\n",
    "**Data:**\n",
    "\n",
    "Target data and source data should come from different domain.\n",
    "\n",
    "**Domain:**\n",
    "\n",
    "target data or source data\n",
    "\n",
    "**structure:**\n",
    "\n",
    "we construct a NN take target data and source data as input, then output a feature vector. This NN is called feature extractor, it aims to eliminate the domain feature.\n",
    "\n",
    "Moreover, we construct a NN take the feature vector as input, then output a domain label. This NN is called Domain classifer, it aims to distinguish the domain of feature vector.\n",
    "\n",
    "Finally, we construct a NN also take the feature vector as input, then output class label for classification. This NN is called Label predictor, it aims to do classifcation.\n",
    "\n",
    "This structure is similar to GAN, feature extractor should try best to cheat Domain classifier and Domain classifier should struggle to distinguish data domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to train\n",
    "\n",
    "We train these three NNs as a whole NN. \n",
    "\n",
    "However, we need to do a little adjust when we do gradient descent. That is, the gradient of feature extractor and the gradient of label predictor are in positive correlation because we want to predict class correctly. \n",
    "\n",
    "Nevertheless,  the gradient of feature extractor and the gradient of domain classifier are in negative correlation because if we update parameters in feature extractor to increase gradient in classifier, that could incrase the loss of feature extractor. Hence we will cheat it finally.\n",
    "\n",
    "To achieve this, an additional layer called 'gradient reversal layer' should be added between the output of feature extractor and the input of domain classifier. It will reverse the gradient computed in domain classifer when do propagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Zero-shot Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### introduction\n",
    "\n",
    "**Data:**\n",
    "\n",
    "Source data and target data should work for different tasks.\n",
    "\n",
    "For example, the label of Source data is set A, the label of target data is set B. Then $A \\cap B = \\emptyset$\n",
    "\n",
    "**How to train**\n",
    "\n",
    "Generally, we represent each class by a vector of attributes.\n",
    "\n",
    "There are two method to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database of attributes\n",
    "\n",
    "Firstly, we need create a database. The index of database is different label and the columns of database is a series of attributes. \n",
    "\n",
    "The number of attributes should be sufficient to make each label has unique attributes tuple. That is, if $Tuple(A) = Tuple(B)$, then $A = B$, if $Tuple(A) \\ne Tuple(B)$, then $A \\ne B$.\n",
    "\n",
    "Secondly, Let's train a NN take inputs and output a vector(Tuple) of attributes. This vecotr(Tuple) could hepl us find which label it should belongs to or be hte most similar to by scan database.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute embedding + word embedding\n",
    "\n",
    "**structure**\n",
    "\n",
    "Similar to last method, we need to train a NN take inputs and output a vector of attributes.\n",
    "\n",
    "However, since there is no database for match checking, we can use the technique of word embedding to project a bunch of words to the same space as attributes. \n",
    "\n",
    "Then the word vector is the label of a data with the greatest similarity between word vector and attributes vector.\n",
    "\n",
    "**Loss function of zero-shot**\n",
    "\n",
    "if we define \n",
    "\n",
    "$$Loss = argmin_{f, g} \\sum_{n}|| f(x^n) - g(y^n)||_2$$\n",
    "\n",
    ", where f(x) give us attributes vector and g(y) give us word embedding of label. It hope to find $f$ and $g$ such that every data and their label has the minimum distance in space. \n",
    "\n",
    "The problem in here is loss could reduce to 0 if whatever input $f$ and $g$ takes, they always output same results.\n",
    "\n",
    "Hence, Loss function should be modified to \n",
    "\n",
    "$$\n",
    "Loss = argmin_{f, g}\\sum_n max(0, k-f(x^n) \\cdot g(y^n) + max_{m \\ne n} f(x^n) \\cdot g(y^m))\n",
    "$$\n",
    "\n",
    "Let's focus on \n",
    "$$\n",
    "k-f(x^n) \\cdot g(y^n) + max_{m \\ne n} f(x^n) \\cdot g(y^m)\n",
    "$$\n",
    "\n",
    "Our goal is to minimise this part as much as possible (less than 0) to decrease loss to 0.\n",
    "\n",
    "That is \n",
    "\n",
    "$$\n",
    "k-f(x^n) \\cdot g(y^n) + max_{m \\ne n} f(x^n) \\cdot g(y^m) < 0\n",
    "$$\n",
    "\n",
    "also is \n",
    "\n",
    "$$\n",
    "f(x^n) \\cdot g(y^n) - max_{m \\ne n} f(x^n) \\cdot g(y^m) > k\n",
    "$$\n",
    "\n",
    "this means we hope the similarity between a data and its real label to be as great as possible. At the same time, the similarity between this data and any other false label to be as less as possible.\n",
    "\n",
    "more make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convex combination of semantic embedding\n",
    "\n",
    "Given a well-trained NN, it takes our data as input, then output a vecotr $P$ which is the probability vector. This vector tell us probability of this data belongs to each class(label).\n",
    "\n",
    "Then given a well-trained word embedding database, we find the vector has the greast similarity with $\\sum_{n} p^n V^n$, where $p^n$ is nth element in $P$, and $V^n$ is the corresponding label in word embedding. This vector with the greast similarity is the predicted label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of zero-shot learning\n",
    "\n",
    "Google Neural Machine Translation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
